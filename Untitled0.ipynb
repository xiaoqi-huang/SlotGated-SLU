{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNywywKbySgolAIgf/BXMbO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaoqi-huang/SlotGated-SLU/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPfwkYHoYEnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def createVocabulary(input_path, output_path, no_pad=False):\n",
        "    if not isinstance(input_path, str):\n",
        "        raise TypeError('input_path should be string')\n",
        "\n",
        "    if not isinstance(output_path, str):\n",
        "        raise TypeError('output_path should be string')\n",
        "\n",
        "    vocab = {}\n",
        "    with open(input_path, 'r') as fd, \\\n",
        "            open(output_path, 'w+') as out:\n",
        "        for line in fd:\n",
        "            line = line.rstrip('\\r\\n')\n",
        "            words = line.split()\n",
        "\n",
        "            for w in words:\n",
        "                if w == '_UNK':\n",
        "                    break\n",
        "                if str.isdigit(w):\n",
        "                    w = '0'\n",
        "                if w in vocab:\n",
        "                    vocab[w] += 1\n",
        "                else:\n",
        "                    vocab[w] = 1\n",
        "        if no_pad == False:\n",
        "            vocab = ['_PAD', '_UNK'] + sorted(vocab, key=vocab.get, reverse=True)\n",
        "        else:\n",
        "            vocab = ['_UNK'] + sorted(vocab, key=vocab.get, reverse=True)\n",
        "\n",
        "        for v in vocab:\n",
        "            out.write(v + '\\n')\n",
        "\n",
        "\n",
        "def loadVocabulary(path):\n",
        "    if not isinstance(path, str):\n",
        "        raise TypeError('path should be a string')\n",
        "\n",
        "    vocab = []\n",
        "    rev = []\n",
        "    with open(path) as fd:\n",
        "        for line in fd:\n",
        "            line = line.rstrip('\\r\\n')\n",
        "            rev.append(line)\n",
        "        vocab = dict([(x, y) for (y, x) in enumerate(rev)])\n",
        "\n",
        "    return {'vocab': vocab, 'rev': rev}\n",
        "\n",
        "\n",
        "def sentenceToIds(data, vocab):\n",
        "    if not isinstance(vocab, dict):\n",
        "        raise TypeError('vocab should be a dict that contains vocab and rev')\n",
        "    vocab = vocab['vocab']\n",
        "    if isinstance(data, str):\n",
        "        words = data.split()\n",
        "    elif isinstance(data, list):\n",
        "        words = data\n",
        "    else:\n",
        "        raise TypeError('data should be a string or a list contains words')\n",
        "\n",
        "    ids = []\n",
        "    for w in words:\n",
        "        if str.isdigit(w) == True:\n",
        "            w = '0'\n",
        "        ids.append(vocab.get(w, vocab['_UNK']))\n",
        "\n",
        "    return ids\n",
        "\n",
        "\n",
        "def padSentence(s, max_length, vocab):\n",
        "    return s + [vocab['vocab']['_PAD']] * (max_length - len(s))\n",
        "\n",
        "\n",
        "# compute f1 score is modified from conlleval.pl\n",
        "def __startOfChunk(prevTag, tag, prevTagType, tagType, chunkStart=False):\n",
        "    if prevTag == 'B' and tag == 'B':\n",
        "        chunkStart = True\n",
        "    if prevTag == 'I' and tag == 'B':\n",
        "        chunkStart = True\n",
        "    if prevTag == 'O' and tag == 'B':\n",
        "        chunkStart = True\n",
        "    if prevTag == 'O' and tag == 'I':\n",
        "        chunkStart = True\n",
        "\n",
        "    if prevTag == 'E' and tag == 'E':\n",
        "        chunkStart = True\n",
        "    if prevTag == 'E' and tag == 'I':\n",
        "        chunkStart = True\n",
        "    if prevTag == 'O' and tag == 'E':\n",
        "        chunkStart = True\n",
        "    if prevTag == 'O' and tag == 'I':\n",
        "        chunkStart = True\n",
        "\n",
        "    if tag != 'O' and tag != '.' and prevTagType != tagType:\n",
        "        chunkStart = True\n",
        "    return chunkStart\n",
        "\n",
        "\n",
        "def __endOfChunk(prevTag, tag, prevTagType, tagType, chunkEnd=False):\n",
        "    if prevTag == 'B' and tag == 'B':\n",
        "        chunkEnd = True\n",
        "    if prevTag == 'B' and tag == 'O':\n",
        "        chunkEnd = True\n",
        "    if prevTag == 'I' and tag == 'B':\n",
        "        chunkEnd = True\n",
        "    if prevTag == 'I' and tag == 'O':\n",
        "        chunkEnd = True\n",
        "\n",
        "    if prevTag == 'E' and tag == 'E':\n",
        "        chunkEnd = True\n",
        "    if prevTag == 'E' and tag == 'I':\n",
        "        chunkEnd = True\n",
        "    if prevTag == 'E' and tag == 'O':\n",
        "        chunkEnd = True\n",
        "    if prevTag == 'I' and tag == 'O':\n",
        "        chunkEnd = True\n",
        "\n",
        "    if prevTag != 'O' and prevTag != '.' and prevTagType != tagType:\n",
        "        chunkEnd = True\n",
        "    return chunkEnd\n",
        "\n",
        "\n",
        "def __splitTagType(tag):\n",
        "    s = tag.split('-')\n",
        "    if len(s) > 2 or len(s) == 0:\n",
        "        raise ValueError('tag format wrong. it must be B-xxx.xxx')\n",
        "    if len(s) == 1:\n",
        "        tag = s[0]\n",
        "        tagType = \"\"\n",
        "    else:\n",
        "        tag = s[0]\n",
        "        tagType = s[1]\n",
        "    return tag, tagType\n",
        "\n",
        "\n",
        "def computeF1Score(correct_slots, pred_slots):\n",
        "    correctChunk = {}\n",
        "    correctChunkCnt = 0\n",
        "    foundCorrect = {}\n",
        "    foundCorrectCnt = 0\n",
        "    foundPred = {}\n",
        "    foundPredCnt = 0\n",
        "    correctTags = 0\n",
        "    tokenCount = 0\n",
        "    for correct_slot, pred_slot in zip(correct_slots, pred_slots):\n",
        "        inCorrect = False\n",
        "        lastCorrectTag = 'O'\n",
        "        lastCorrectType = ''\n",
        "        lastPredTag = 'O'\n",
        "        lastPredType = ''\n",
        "        for c, p in zip(correct_slot, pred_slot):\n",
        "            correctTag, correctType = __splitTagType(c)\n",
        "            predTag, predType = __splitTagType(p)\n",
        "\n",
        "            if inCorrect == True:\n",
        "                if __endOfChunk(lastCorrectTag, correctTag, lastCorrectType, correctType) == True and \\\n",
        "                        __endOfChunk(lastPredTag, predTag, lastPredType, predType) == True and \\\n",
        "                        (lastCorrectType == lastPredType):\n",
        "                    inCorrect = False\n",
        "                    correctChunkCnt += 1\n",
        "                    if lastCorrectType in correctChunk:\n",
        "                        correctChunk[lastCorrectType] += 1\n",
        "                    else:\n",
        "                        correctChunk[lastCorrectType] = 1\n",
        "                elif __endOfChunk(lastCorrectTag, correctTag, lastCorrectType, correctType) != \\\n",
        "                        __endOfChunk(lastPredTag, predTag, lastPredType, predType) or \\\n",
        "                        (correctType != predType):\n",
        "                    inCorrect = False\n",
        "\n",
        "            if __startOfChunk(lastCorrectTag, correctTag, lastCorrectType, correctType) == True and \\\n",
        "                    __startOfChunk(lastPredTag, predTag, lastPredType, predType) == True and \\\n",
        "                    (correctType == predType):\n",
        "                inCorrect = True\n",
        "\n",
        "            if __startOfChunk(lastCorrectTag, correctTag, lastCorrectType, correctType) == True:\n",
        "                foundCorrectCnt += 1\n",
        "                if correctType in foundCorrect:\n",
        "                    foundCorrect[correctType] += 1\n",
        "                else:\n",
        "                    foundCorrect[correctType] = 1\n",
        "\n",
        "            if __startOfChunk(lastPredTag, predTag, lastPredType, predType) == True:\n",
        "                foundPredCnt += 1\n",
        "                if predType in foundPred:\n",
        "                    foundPred[predType] += 1\n",
        "                else:\n",
        "                    foundPred[predType] = 1\n",
        "\n",
        "            if correctTag == predTag and correctType == predType:\n",
        "                correctTags += 1\n",
        "\n",
        "            tokenCount += 1\n",
        "\n",
        "            lastCorrectTag = correctTag\n",
        "            lastCorrectType = correctType\n",
        "            lastPredTag = predTag\n",
        "            lastPredType = predType\n",
        "\n",
        "        if inCorrect == True:\n",
        "            correctChunkCnt += 1\n",
        "            if lastCorrectType in correctChunk:\n",
        "                correctChunk[lastCorrectType] += 1\n",
        "            else:\n",
        "                correctChunk[lastCorrectType] = 1\n",
        "\n",
        "    if foundPredCnt > 0:\n",
        "        precision = 100 * correctChunkCnt / foundPredCnt\n",
        "    else:\n",
        "        precision = 0\n",
        "\n",
        "    if foundCorrectCnt > 0:\n",
        "        recall = 100 * correctChunkCnt / foundCorrectCnt\n",
        "    else:\n",
        "        recall = 0\n",
        "\n",
        "    if (precision + recall) > 0:\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "    else:\n",
        "        f1 = 0\n",
        "\n",
        "    return f1, precision, recall\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    def __init__(self, in_path, slot_path, intent_path, in_vocab, slot_vocab, intent_vocab):\n",
        "        self.__fd_in = open(in_path, 'r')\n",
        "        self.__fd_slot = open(slot_path, 'r')\n",
        "        self.__fd_intent = open(intent_path, 'r')\n",
        "        self.__in_vocab = in_vocab\n",
        "        self.__slot_vocab = slot_vocab\n",
        "        self.__intent_vocab = intent_vocab\n",
        "        self.end = 0\n",
        "\n",
        "    def close(self):\n",
        "        self.__fd_in.close()\n",
        "        self.__fd_slot.close()\n",
        "        self.__fd_intent.close()\n",
        "\n",
        "    def get_batch(self, batch_size):\n",
        "        in_data = []\n",
        "        slot_data = []\n",
        "        slot_weight = []\n",
        "        length = []\n",
        "        intents = []\n",
        "\n",
        "        batch_in = []\n",
        "        batch_slot = []\n",
        "        max_len = 0\n",
        "\n",
        "        # used to record word(not id)\n",
        "        in_seq = []\n",
        "        slot_seq = []\n",
        "        intent_seq = []\n",
        "        for i in range(batch_size):\n",
        "            inp = self.__fd_in.readline()\n",
        "            if inp == '':\n",
        "                self.end = 1\n",
        "                break\n",
        "            slot = self.__fd_slot.readline()\n",
        "            intent = self.__fd_intent.readline()\n",
        "            inp = inp.rstrip()\n",
        "            slot = slot.rstrip()\n",
        "            intent = intent.rstrip()\n",
        "\n",
        "            in_seq.append(inp)\n",
        "            slot_seq.append(slot)\n",
        "            intent_seq.append(intent)\n",
        "\n",
        "            iii = inp\n",
        "            sss = slot\n",
        "            inp = sentenceToIds(inp, self.__in_vocab)\n",
        "            slot = sentenceToIds(slot, self.__slot_vocab)\n",
        "            intent = sentenceToIds(intent, self.__intent_vocab)\n",
        "            batch_in.append(np.array(inp))\n",
        "            batch_slot.append(np.array(slot))\n",
        "            length.append(len(inp))\n",
        "            intents.append(intent[0])\n",
        "            if len(inp) != len(slot):\n",
        "                print(iii, sss)\n",
        "                print(inp, slot)\n",
        "                exit(0)\n",
        "            if len(inp) > max_len:\n",
        "                max_len = len(inp)\n",
        "\n",
        "        length = np.array(length)\n",
        "        intents = np.array(intents)\n",
        "        # print(max_len)\n",
        "        # print('A'*20)\n",
        "        for i, s in zip(batch_in, batch_slot):\n",
        "            in_data.append(padSentence(list(i), max_len, self.__in_vocab))\n",
        "            slot_data.append(padSentence(list(s), max_len, self.__slot_vocab))\n",
        "            # print(s)\n",
        "        in_data = np.array(in_data)\n",
        "        slot_data = np.array(slot_data)\n",
        "        # print(in_data)\n",
        "        # print(slot_data)\n",
        "        # print(type(slot_data))\n",
        "        for s in slot_data:\n",
        "            weight = np.not_equal(s, np.zeros(s.shape))\n",
        "            weight = weight.astype(np.float32)\n",
        "            slot_weight.append(weight)\n",
        "        slot_weight = np.array(slot_weight)\n",
        "        return in_data, slot_data, slot_weight, length, intents, in_seq, slot_seq, intent_seq\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FET9UbSdZMAe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "3cd50807-08a5-4d89-815e-201eb1fe9233"
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.python.ops import rnn_cell_impl\n",
        "\n",
        "parser = argparse.ArgumentParser(allow_abbrev=False)\n",
        "\n",
        "# Network\n",
        "parser.add_argument(\"--num_units\", type=int, default=64, help=\"Network size.\", dest='layer_size')\n",
        "parser.add_argument(\"--model_type\", type=str, default='full', help=\"\"\"full(default) | intent_only\n",
        "                                                                    full: full attention model\n",
        "                                                                    intent_only: intent attention model\"\"\")\n",
        "\n",
        "# Training Environment\n",
        "parser.add_argument(\"--batch_size\", type=int, default=16, help=\"Batch size.\")\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=20, help=\"Max epochs to train.\")\n",
        "parser.add_argument(\"--no_early_stop\", action='store_false', dest='early_stop',\n",
        "                    help=\"Disable early stop, which is based on sentence level accuracy.\")\n",
        "parser.add_argument(\"--patience\", type=int, default=5, help=\"Patience to wait before stop.\")\n",
        "\n",
        "# Model and Vocab\n",
        "parser.add_argument(\"--dataset\", type=str, default=None, help=\"\"\"Type 'atis' or 'snips' to use dataset provided by us or enter what ever you named your own dataset.\n",
        "                Note, if you don't want to use this part, enter --dataset=''. It can not be None\"\"\")\n",
        "parser.add_argument(\"--model_path\", type=str, default='./model', help=\"Path to save model.\")\n",
        "parser.add_argument(\"--vocab_path\", type=str, default='./vocab', help=\"Path to vocabulary files.\")\n",
        "\n",
        "# Data\n",
        "parser.add_argument(\"--train_data_path\", type=str, default='train', help=\"Path to training data files.\")\n",
        "parser.add_argument(\"--test_data_path\", type=str, default='test', help=\"Path to testing data files.\")\n",
        "parser.add_argument(\"--valid_data_path\", type=str, default='valid', help=\"Path to validation data files.\")\n",
        "parser.add_argument(\"--input_file\", type=str, default='seq.in', help=\"Input file name.\")\n",
        "parser.add_argument(\"--slot_file\", type=str, default='seq.out', help=\"Slot file name.\")\n",
        "parser.add_argument(\"--intent_file\", type=str, default='label', help=\"Intent file name.\")\n",
        "\n",
        "arg = parser.parse_args()\n",
        "\n",
        "# Print arguments\n",
        "for k, v in sorted(vars(arg).items()):\n",
        "    print(k, '=', v)\n",
        "print()\n",
        "\n",
        "if arg.model_type == 'full':\n",
        "    add_final_state_to_intent = True\n",
        "    remove_slot_attn = False\n",
        "elif arg.model_type == 'intent_only':\n",
        "    add_final_state_to_intent = True\n",
        "    remove_slot_attn = True\n",
        "else:\n",
        "    print('unknown model type!')\n",
        "    exit(1)\n",
        "\n",
        "# full path to data will be: ./data + dataset + train/test/valid\n",
        "if arg.dataset == None:\n",
        "    print('name of dataset can not be None')\n",
        "    exit(1)\n",
        "elif arg.dataset == 'snips':\n",
        "    print('use snips dataset')\n",
        "elif arg.dataset == 'atis':\n",
        "    print('use atis dataset')\n",
        "else:\n",
        "    print('use own dataset: ', arg.dataset)\n",
        "full_train_path = os.path.join('./data', arg.dataset, arg.train_data_path)\n",
        "full_test_path = os.path.join('./data', arg.dataset, arg.test_data_path)\n",
        "full_valid_path = os.path.join('./data', arg.dataset, arg.valid_data_path)\n",
        "\n",
        "createVocabulary(os.path.join(full_train_path, arg.input_file), os.path.join(arg.vocab_path, 'in_vocab'))\n",
        "createVocabulary(os.path.join(full_train_path, arg.slot_file), os.path.join(arg.vocab_path, 'slot_vocab'))\n",
        "createVocabulary(os.path.join(full_train_path, arg.intent_file), os.path.join(arg.vocab_path, 'intent_vocab'))\n",
        "\n",
        "in_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'in_vocab'))\n",
        "slot_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'slot_vocab'))\n",
        "intent_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'intent_vocab'))\n",
        "\n",
        "\n",
        "def createModel(input_data, input_size, sequence_length, slot_size, intent_size, layer_size=128, isTraining=True):\n",
        "    cell_fw = tf.contrib.rnn.BasicLSTMCell(layer_size)\n",
        "    cell_bw = tf.contrib.rnn.BasicLSTMCell(layer_size)\n",
        "\n",
        "    if isTraining == True:\n",
        "        cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob=0.5,\n",
        "                                                output_keep_prob=0.5)\n",
        "        cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob=0.5,\n",
        "                                                output_keep_prob=0.5)\n",
        "\n",
        "    embedding = tf.get_variable('embedding', [input_size, layer_size])\n",
        "    inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
        "\n",
        "    state_outputs, final_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs,\n",
        "                                                                 sequence_length=sequence_length, dtype=tf.float32)\n",
        "\n",
        "    final_state = tf.concat([final_state[0][0], final_state[0][1], final_state[1][0], final_state[1][1]], 1)\n",
        "    state_outputs = tf.concat([state_outputs[0], state_outputs[1]], 2)\n",
        "    state_shape = state_outputs.get_shape()\n",
        "\n",
        "    with tf.variable_scope('attention'):\n",
        "        slot_inputs = state_outputs\n",
        "        if remove_slot_attn == False:\n",
        "            with tf.variable_scope('slot_attn'):\n",
        "                attn_size = state_shape[2].value\n",
        "                origin_shape = tf.shape(state_outputs)\n",
        "                hidden = tf.expand_dims(state_outputs, 1)\n",
        "                hidden_conv = tf.expand_dims(state_outputs, 2)\n",
        "                # hidden shape = [batch, sentence length, 1, hidden size]\n",
        "                k = tf.get_variable(\"AttnW\", [1, 1, attn_size, attn_size])\n",
        "                hidden_features = tf.nn.conv2d(hidden_conv, k, [1, 1, 1, 1], \"SAME\")\n",
        "                hidden_features = tf.reshape(hidden_features, origin_shape)\n",
        "                hidden_features = tf.expand_dims(hidden_features, 1)\n",
        "                v = tf.get_variable(\"AttnV\", [attn_size])\n",
        "\n",
        "                slot_inputs_shape = tf.shape(slot_inputs)\n",
        "                slot_inputs = tf.reshape(slot_inputs, [-1, attn_size])\n",
        "                y = rnn_cell_impl._linear(slot_inputs, attn_size, True)\n",
        "                y = tf.reshape(y, slot_inputs_shape)\n",
        "                y = tf.expand_dims(y, 2)\n",
        "                s = tf.reduce_sum(v * tf.tanh(hidden_features + y), [3])\n",
        "                a = tf.nn.softmax(s)\n",
        "                # a shape = [batch, input size, sentence length, 1]\n",
        "                a = tf.expand_dims(a, -1)\n",
        "                slot_d = tf.reduce_sum(a * hidden, [2])\n",
        "        else:\n",
        "            attn_size = state_shape[2].value\n",
        "            slot_inputs = tf.reshape(slot_inputs, [-1, attn_size])\n",
        "\n",
        "        intent_input = final_state\n",
        "        with tf.variable_scope('intent_attn'):\n",
        "            attn_size = state_shape[2].value\n",
        "            hidden = tf.expand_dims(state_outputs, 2)\n",
        "            k = tf.get_variable(\"AttnW\", [1, 1, attn_size, attn_size])\n",
        "            hidden_features = tf.nn.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\")\n",
        "            v = tf.get_variable(\"AttnV\", [attn_size])\n",
        "\n",
        "            y = rnn_cell_impl._linear(intent_input, attn_size, True)\n",
        "            y = tf.reshape(y, [-1, 1, 1, attn_size])\n",
        "            s = tf.reduce_sum(v * tf.tanh(hidden_features + y), [2, 3])\n",
        "            a = tf.nn.softmax(s)\n",
        "            a = tf.expand_dims(a, -1)\n",
        "            a = tf.expand_dims(a, -1)\n",
        "            d = tf.reduce_sum(a * hidden, [1, 2])\n",
        "\n",
        "            if add_final_state_to_intent == True:\n",
        "                intent_output = tf.concat([d, intent_input], 1)\n",
        "            else:\n",
        "                intent_output = d\n",
        "\n",
        "        with tf.variable_scope('slot_gated'):\n",
        "            intent_gate = rnn_cell_impl._linear(intent_output, attn_size, True)\n",
        "            intent_gate = tf.reshape(intent_gate, [-1, 1, intent_gate.get_shape()[1].value])\n",
        "            v1 = tf.get_variable(\"gateV\", [attn_size])\n",
        "            if remove_slot_attn == False:\n",
        "                slot_gate = v1 * tf.tanh(slot_d + intent_gate)\n",
        "            else:\n",
        "                slot_gate = v1 * tf.tanh(state_outputs + intent_gate)\n",
        "            slot_gate = tf.reduce_sum(slot_gate, [2])\n",
        "            slot_gate = tf.expand_dims(slot_gate, -1)\n",
        "            if remove_slot_attn == False:\n",
        "                slot_gate = slot_d * slot_gate\n",
        "            else:\n",
        "                slot_gate = state_outputs * slot_gate\n",
        "            slot_gate = tf.reshape(slot_gate, [-1, attn_size])\n",
        "            slot_output = tf.concat([slot_gate, slot_inputs], 1)\n",
        "\n",
        "    with tf.variable_scope('intent_proj'):\n",
        "        intent = rnn_cell_impl._linear(intent_output, intent_size, True)\n",
        "\n",
        "    with tf.variable_scope('slot_proj'):\n",
        "        slot = rnn_cell_impl._linear(slot_output, slot_size, True)\n",
        "\n",
        "    outputs = [slot, intent]\n",
        "    return outputs\n",
        "\n",
        "\n",
        "# Create Training Model\n",
        "input_data = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
        "sequence_length = tf.placeholder(tf.int32, [None], name=\"sequence_length\")\n",
        "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "slots = tf.placeholder(tf.int32, [None, None], name='slots')\n",
        "slot_weights = tf.placeholder(tf.float32, [None, None], name='slot_weights')\n",
        "intent = tf.placeholder(tf.int32, [None], name='intent')\n",
        "\n",
        "with tf.variable_scope('model'):\n",
        "    training_outputs = createModel(input_data, len(in_vocab['vocab']), sequence_length, len(slot_vocab['vocab']),\n",
        "                                   len(intent_vocab['vocab']), layer_size=arg.layer_size)\n",
        "\n",
        "slots_shape = tf.shape(slots)\n",
        "slots_reshape = tf.reshape(slots, [-1])\n",
        "\n",
        "slot_outputs = training_outputs[0]\n",
        "with tf.variable_scope('slot_loss'):\n",
        "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=slots_reshape, logits=slot_outputs)\n",
        "    crossent = tf.reshape(crossent, slots_shape)\n",
        "    slot_loss = tf.reduce_sum(crossent * slot_weights, 1)\n",
        "    total_size = tf.reduce_sum(slot_weights, 1)\n",
        "    total_size += 1e-12\n",
        "    slot_loss = slot_loss / total_size\n",
        "\n",
        "intent_output = training_outputs[1]\n",
        "with tf.variable_scope('intent_loss'):\n",
        "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=intent, logits=intent_output)\n",
        "    intent_loss = tf.reduce_sum(crossent) / tf.cast(arg.batch_size, tf.float32)\n",
        "\n",
        "params = tf.trainable_variables()\n",
        "opt = tf.train.AdamOptimizer()\n",
        "\n",
        "intent_params = []\n",
        "slot_params = []\n",
        "for p in params:\n",
        "    if not 'slot_' in p.name:\n",
        "        intent_params.append(p)\n",
        "    if 'slot_' in p.name or 'bidirectional_rnn' in p.name or 'embedding' in p.name:\n",
        "        slot_params.append(p)\n",
        "\n",
        "gradients_slot = tf.gradients(slot_loss, slot_params)\n",
        "gradients_intent = tf.gradients(intent_loss, intent_params)\n",
        "\n",
        "clipped_gradients_slot, norm_slot = tf.clip_by_global_norm(gradients_slot, 5.0)\n",
        "clipped_gradients_intent, norm_intent = tf.clip_by_global_norm(gradients_intent, 5.0)\n",
        "\n",
        "gradient_norm_slot = norm_slot\n",
        "gradient_norm_intent = norm_intent\n",
        "update_slot = opt.apply_gradients(zip(clipped_gradients_slot, slot_params))\n",
        "update_intent = opt.apply_gradients(zip(clipped_gradients_intent, intent_params), global_step=global_step)\n",
        "\n",
        "training_outputs = [global_step, slot_loss, update_intent, update_slot, gradient_norm_intent, gradient_norm_slot]\n",
        "inputs = [input_data, sequence_length, slots, slot_weights, intent]\n",
        "\n",
        "# Create Inference Model\n",
        "with tf.variable_scope('model', reuse=True):\n",
        "    inference_outputs = createModel(input_data, len(in_vocab['vocab']), sequence_length, len(slot_vocab['vocab']),\n",
        "                                    len(intent_vocab['vocab']), layer_size=arg.layer_size, isTraining=False)\n",
        "\n",
        "inference_slot_output = tf.nn.softmax(inference_outputs[0], name='slot_output')\n",
        "inference_intent_output = tf.nn.softmax(inference_outputs[1], name='intent_output')\n",
        "\n",
        "inference_outputs = [inference_intent_output, inference_slot_output]\n",
        "inference_inputs = [input_data, sequence_length]\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# Start Training\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    logging.info('Training Start')\n",
        "\n",
        "    epochs = 0\n",
        "    loss = 0.0\n",
        "    data_processor = None\n",
        "    line = 0\n",
        "    num_loss = 0\n",
        "    step = 0\n",
        "    no_improve = 0\n",
        "\n",
        "    # variables to store highest values among epochs, only use 'valid_err' for now\n",
        "    valid_slot = 0\n",
        "    test_slot = 0\n",
        "    valid_intent = 0\n",
        "    test_intent = 0\n",
        "    valid_err = 0\n",
        "    test_err = 0\n",
        "\n",
        "    while True:\n",
        "        if data_processor == None:\n",
        "            data_processor = DataProcessor(os.path.join(full_train_path, arg.input_file),\n",
        "                                           os.path.join(full_train_path, arg.slot_file),\n",
        "                                           os.path.join(full_train_path, arg.intent_file), in_vocab, slot_vocab,\n",
        "                                           intent_vocab)\n",
        "        in_data, slot_data, slot_weight, length, intents, _, _, _ = data_processor.get_batch(arg.batch_size)\n",
        "        feed_dict = {input_data.name: in_data, slots.name: slot_data, slot_weights.name: slot_weight,\n",
        "                     sequence_length.name: length, intent.name: intents}\n",
        "        ret = sess.run(training_outputs, feed_dict)\n",
        "        loss += np.mean(ret[1])\n",
        "\n",
        "        line += arg.batch_size\n",
        "        step = ret[0]\n",
        "        num_loss += 1\n",
        "\n",
        "        if data_processor.end == 1:\n",
        "            line = 0\n",
        "            data_processor.close()\n",
        "            data_processor = None\n",
        "            epochs += 1\n",
        "            logging.info('Step: ' + str(step))\n",
        "            logging.info('Epochs: ' + str(epochs))\n",
        "            logging.info('Loss: ' + str(loss / num_loss))\n",
        "            num_loss = 0\n",
        "            loss = 0.0\n",
        "\n",
        "            save_path = os.path.join(arg.model_path, '_step_' + str(step) + '_epochs_' + str(epochs) + '.ckpt')\n",
        "            saver.save(sess, save_path)\n",
        "\n",
        "\n",
        "            def valid(in_path, slot_path, intent_path):\n",
        "                data_processor_valid = DataProcessor(in_path, slot_path, intent_path, in_vocab, slot_vocab,\n",
        "                                                     intent_vocab)\n",
        "\n",
        "                pred_intents = []\n",
        "                correct_intents = []\n",
        "                slot_outputs = []\n",
        "                correct_slots = []\n",
        "                input_words = []\n",
        "\n",
        "                # used to gate\n",
        "                gate_seq = []\n",
        "                while True:\n",
        "                    in_data, slot_data, slot_weight, length, intents, in_seq, slot_seq, intent_seq = data_processor_valid.get_batch(\n",
        "                        arg.batch_size)\n",
        "                    feed_dict = {input_data.name: in_data, sequence_length.name: length}\n",
        "                    ret = sess.run(inference_outputs, feed_dict)\n",
        "                    for i in ret[0]:\n",
        "                        pred_intents.append(np.argmax(i))\n",
        "                    for i in intents:\n",
        "                        correct_intents.append(i)\n",
        "\n",
        "                    pred_slots = ret[1].reshape((slot_data.shape[0], slot_data.shape[1], -1))\n",
        "                    for p, t, i, l in zip(pred_slots, slot_data, in_data, length):\n",
        "                        p = np.argmax(p, 1)\n",
        "                        tmp_pred = []\n",
        "                        tmp_correct = []\n",
        "                        tmp_input = []\n",
        "                        for j in range(l):\n",
        "                            tmp_pred.append(slot_vocab['rev'][p[j]])\n",
        "                            tmp_correct.append(slot_vocab['rev'][t[j]])\n",
        "                            tmp_input.append(in_vocab['rev'][i[j]])\n",
        "\n",
        "                        slot_outputs.append(tmp_pred)\n",
        "                        correct_slots.append(tmp_correct)\n",
        "                        input_words.append(tmp_input)\n",
        "\n",
        "                    if data_processor_valid.end == 1:\n",
        "                        break\n",
        "\n",
        "                pred_intents = np.array(pred_intents)\n",
        "                correct_intents = np.array(correct_intents)\n",
        "                accuracy = (pred_intents == correct_intents)\n",
        "                semantic_error = accuracy\n",
        "                accuracy = accuracy.astype(float)\n",
        "                accuracy = np.mean(accuracy) * 100.0\n",
        "\n",
        "                index = 0\n",
        "                for t, p in zip(correct_slots, slot_outputs):\n",
        "                    # Process Semantic Error\n",
        "                    if len(t) != len(p):\n",
        "                        raise ValueError('Error!!')\n",
        "\n",
        "                    for j in range(len(t)):\n",
        "                        if p[j] != t[j]:\n",
        "                            semantic_error[index] = False\n",
        "                            break\n",
        "                    index += 1\n",
        "                semantic_error = semantic_error.astype(float)\n",
        "                semantic_error = np.mean(semantic_error) * 100.0\n",
        "\n",
        "                f1, precision, recall = computeF1Score(correct_slots, slot_outputs)\n",
        "                logging.info('slot f1: ' + str(f1))\n",
        "                logging.info('intent accuracy: ' + str(accuracy))\n",
        "                logging.info('semantic error(intent, slots are all correct): ' + str(semantic_error))\n",
        "\n",
        "                data_processor_valid.close()\n",
        "                return f1, accuracy, semantic_error, pred_intents, correct_intents, slot_outputs, correct_slots, input_words, gate_seq\n",
        "\n",
        "\n",
        "            logging.info('Valid:')\n",
        "            epoch_valid_slot, epoch_valid_intent, epoch_valid_err, valid_pred_intent, valid_correct_intent, valid_pred_slot, valid_correct_slot, valid_words, valid_gate = valid(\n",
        "                os.path.join(full_valid_path, arg.input_file), os.path.join(full_valid_path, arg.slot_file),\n",
        "                os.path.join(full_valid_path, arg.intent_file))\n",
        "\n",
        "            logging.info('Test:')\n",
        "            epoch_test_slot, epoch_test_intent, epoch_test_err, test_pred_intent, test_correct_intent, test_pred_slot, test_correct_slot, test_words, test_gate = valid(\n",
        "                os.path.join(full_test_path, arg.input_file), os.path.join(full_test_path, arg.slot_file),\n",
        "                os.path.join(full_test_path, arg.intent_file))\n",
        "\n",
        "            if epoch_valid_err <= valid_err:\n",
        "                no_improve += 1\n",
        "            else:\n",
        "                valid_err = epoch_valid_err\n",
        "                no_improve = 0\n",
        "\n",
        "            if epochs == arg.max_epochs:\n",
        "                break\n",
        "\n",
        "            if arg.early_stop == True:\n",
        "                if no_improve > arg.patience:\n",
        "                    break\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--num_units LAYER_SIZE]\n",
            "                             [--model_type MODEL_TYPE]\n",
            "                             [--batch_size BATCH_SIZE]\n",
            "                             [--max_epochs MAX_EPOCHS] [--no_early_stop]\n",
            "                             [--patience PATIENCE] [--dataset DATASET]\n",
            "                             [--model_path MODEL_PATH]\n",
            "                             [--vocab_path VOCAB_PATH]\n",
            "                             [--train_data_path TRAIN_DATA_PATH]\n",
            "                             [--test_data_path TEST_DATA_PATH]\n",
            "                             [--valid_data_path VALID_DATA_PATH]\n",
            "                             [--input_file INPUT_FILE] [--slot_file SLOT_FILE]\n",
            "                             [--intent_file INTENT_FILE]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-a37d0af8-427d-459b-8d3f-157e5ad24a86.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}